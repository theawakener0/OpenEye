# OpenEye configuration file
# Copy or modify this file to suit your local environment. Values shown here match the runtime defaults.

runtime:
  backend: native
  native:
    model_path: "models/LFM2.5-1.2B-Instruct-Q4_K_M.gguf"
    # mmproj_path: "models/mmproj-SmolVLM2-2.2B-Instruct-Q8_0.gguf"  # Vision projector for multimodal (optional)
    context_size: 4096                               # INCREASED: Larger context window for unlimited feel
    threads: 4
    batch_size: 1024                                 # ADJUSTED: Must be < context_size, optimized for throughput
    # threads_batch: 4                                # Threads for batch processing (defaults to threads)
    warmup: true
    # --- Inference Optimizations ---
    kv_cache_type: "q4_0"                            # KV cache quantization: f16, q8_0, q4_0
    stream_chunk_size: 1                            # CHANGED: Lower latency for real-time feel (was 3)
    context_shift: true                             # Auto-shift KV cache when context fills
    
    context_shift_threshold: 0.60                    # Proactive shift at 60% (was 75%) - prevents crashes
    context_shift_discard: 1024                      # Tokens to discard per shift (larger = fewer shifts)
    context_reserve_ratio: 0.25                      # Reserve 25% of context for new prompts/generation
    auto_recover_full_kv: true                       # Automatically recover from KV cache full errors
    max_shift_attempts: 3                            # Maximum retry attempts for context operations
    # draft_model_path: "models/SmolLM2-135M-Instruct-Q4_K_M.gguf"  # DISABLED: Causes sync issues with context clearing
    # speculative_n: 5
  http:
    base_url: "http://127.0.0.1:42069"
    timeout: "0"
  defaults:
    max_tokens: 512
    temperature: 0.2
    top_k: 40
    top_p: 0.9
    min_p: 0.05
    repeat_penalty: 1.1
    repeat_last_n: 64
    stop: []

# Image processing configuration for multimodal models
image:
  enabled: true
  max_width: 512
  max_height: 512
  quality: 75         # Slightly lower quality = faster encode
  output_as_base64: false  # Must be false for native vision models (passes file paths instead)

memory:
  path: "openeye_memory.db"
  turns_to_use: 8                                       # INCREASED: Keep more recent turns in working memory
  
  # Vector Memory Engine (DuckDB-backed) - LEGACY
  # Enables semantic search and long-term memory for SLMs
  vector_enabled: false
  vector_db_path: "openeye_vector.duckdb"
  embedding_dim: 384                    # Embedding dimension (384 for small models, 768 for larger)
  
  # Context Window Management
  max_context_tokens: 8192  
  reserved_for_prompt: 1024
  reserved_for_summary: 512
  
  # Retrieval Settings - OPTIMIZED for recall
  min_similarity: 0.25                                  # LOWERED: Retrieve more potentially relevant facts
  sliding_window_size: 100                              # INCREASED: Consider more candidates
  recency_weight: 0.4                                   # INCREASED: Favor recent memories slightly more
  relevance_weight: 0.6                                 # Balanced with recency
  
  # Memory Compression (reduces storage and improves retrieval)
  compression_enabled: true             # Enable automatic memory compression
  compression_age: "24h"                # Compress memories older than this duration
  compress_batch_size: 10               # Number of memories to compress at once
  auto_compress: true                   # Automatically compress during normal operation
  compress_every_n: 50                  # Trigger compression every N inserts

  # ============================================================================
  # Mem0-Style Long-Term Memory (NEW)
  # ============================================================================
  # Intelligent fact extraction, memory updates (ADD/UPDATE/DELETE/NOOP),
  # entity-relationship graph, and hybrid retrieval optimized for SLMs.
  # 
  # Based on mem0 research: +26% accuracy, 91% lower latency, 90% fewer tokens
  # ============================================================================
  mem0:
    enabled: false
    
    # Storage settings
    storage:
      db_path: "openeye_mem0.duckdb"    # Path to mem0 database
      embedding_dim: 384                 # Embedding dimension
      max_facts: 10000                   # Maximum facts to retain
      prune_threshold: 12000             # Trigger pruning above this count
      prune_keep_recent: 5000            # Keep this many recent facts when pruning
    
    # Fact Extraction (LLM-based)
    extraction:
      enabled: false
      batch_size: 3                      # Conversation turns to process together
      async: true                        # Run extraction in background
      min_text_length: 10                # Minimum text length to process
      max_facts_per_extraction: 10       # Max facts per extraction batch
      extract_entities: true             # Extract entities from text
      extract_relationships: true        # Extract relationships between entities
    
    # Memory Update Operations (ADD/UPDATE/DELETE/NOOP)
    updates:
      enabled: false
      conflict_threshold: 0.85           # Similarity threshold for conflict detection
      top_similar_count: 5               # Number of similar facts to compare
      auto_resolve_conflicts: true       # Automatically resolve contradictions
      track_supersession: true           # Track which facts replace others
    
    # Entity-Relationship Graph
    graph:
      enabled: false
      entity_resolution: true            # Merge similar entities
      entity_similarity_threshold: 0.9   # Threshold for entity resolution
      max_hops: 2                         # Maximum graph traversal depth
      track_relationship_history: false  # Keep history of relationship changes
    
    # Hybrid Retrieval (semantic + importance + recency + access frequency)
    retrieval:
      semantic_weight: 0.50              # Weight for semantic similarity
      importance_weight: 0.25            # Weight for fact importance
      recency_weight: 0.15               # Weight for recency
      access_frequency_weight: 0.10      # Weight for access frequency
      min_score: 0.3                     # Minimum score threshold
      max_results: 20                    # Maximum results to return
      include_graph_results: true        # Include graph traversal results
      graph_results_weight: 0.3          # Weight for graph-based results
      recency_half_life_hours: 168       # Half-life for recency decay (1 week)
    
    # Rolling Summary (async background refresh)
    summary:
      enabled: false
      refresh_interval: "5m"             # How often to refresh summary
      max_facts: 50                       # Max facts to include in summary
      max_tokens: 512                     # Max tokens for summary
      async: true                         # Refresh in background

  # ============================================================================
  # Omem - OpenEye Memory (NEXT-GEN) - ENABLED for unlimited context feel
  # ============================================================================
  # Advanced long-term memory combining SimpleMem, HippoRAG, and Zep techniques.
  # Designed specifically for on-device SLMs (200M-3B parameters).
  # 
  # Key innovations:
  # - Atomic encoding with coreference resolution and temporal anchoring
  # - Multi-view indexing: semantic (dense) + lexical (BM25) + symbolic (metadata)
  # - Zero-LLM retrieval path with rule-based complexity estimation
  # - Episode/session management for cross-conversation continuity
  # ============================================================================
  omem:
    enabled: true                                       # ENABLED: Retains facts even when KV cache shifts
    
    # Storage settings (DuckDB-backed)
    storage:
      db_path: "openeye_omem.duckdb"     # Path to Omem database
      max_facts: 10000                    # Maximum facts to retain
      prune_threshold: 12000              # Trigger pruning above this count
      prune_keep_recent: 5000             # Keep this many recent facts when pruning
      enable_fts: true                    # Enable DuckDB Full-Text Search for BM25
    
    # Atomic Encoder (SimpleMem-inspired) - OPTIMIZED for comprehensive memory
    atomic_encoder:
      enabled: true
      enable_coreference: true            # Resolve "he/she/it" → actual names
      enable_temporal: true               # Anchor "yesterday" → "2026-01-16"
      max_facts_per_turn: 15              # INCREASED: Extract more facts per turn
      min_fact_importance: 0.2            # LOWERED: Keep more facts (filter less aggressively)
      min_text_length: 5                  # LOWERED: Process shorter utterances
      use_llm_for_complex: true           # Use LLM for complex disambiguation
    
    # Multi-View Indexing (Hybrid Retrieval)
    # Three complementary views for maximum recall
    multi_view:
      enabled: true
      semantic_weight: 0.5                # Weight for embedding similarity (0-1)
      lexical_weight: 0.3                 # Weight for BM25 keyword matching (0-1)
      symbolic_weight: 0.2                # Weight for metadata filters (0-1)
      bm25_k1: 1.2                         # BM25 term frequency saturation
      bm25_b: 0.75                         # BM25 length normalization
      extract_keywords: true              # Extract keywords for BM25 index
      max_keywords_per_fact: 20           # Maximum keywords per fact
    
    # Entity Graph (Lightweight HippoRAG-inspired)
    # Connects facts through shared entities and relationships
    entity_graph:
      enabled: true
      max_hops: 1                          # Graph traversal depth (1 = fast, 2+ = thorough)
      entity_resolution: true             # Merge "John" and "John Smith"
      similarity_threshold: 0.85          # Threshold for entity resolution
      use_regex_extraction: true          # Use regex vs LLM for entity extraction
      graph_boost_weight: 0.2             # Boost for graph-connected facts
    
    # Adaptive Retrieval (Complexity-Aware) - OPTIMIZED for unlimited context
    retrieval:
      default_top_k: 10                   # INCREASED: Retrieve more facts by default
      complexity_delta: 3.0               # INCREASED: More aggressive scaling with complexity
      max_top_k: 30                       # INCREASED: Higher maximum for complex queries
      min_score: 0.25                     # LOWERED: Include more marginally relevant facts
      max_context_tokens: 1500            # INCREASED: More context from retrieved memories
      recency_half_life_hours: 168        # Recency decay half-life (168h = 1 week)
      importance_weight: 0.2              # INCREASED: Prioritize important facts
      recency_weight: 0.15                # INCREASED: Slightly favor recent facts
      access_frequency_weight: 0.1        # INCREASED: Prioritize frequently accessed memories
      enable_complexity_estimation: true  # Enable zero-LLM query analysis
    
    # Episode/Session Tracking (Zep-inspired)
    # Maintains context across conversation sessions
    episodes:
      enabled: true
      session_timeout: "30m"              # New episode after this inactivity
      summary_on_close: true              # Generate summary when session ends
      max_episodes_in_cache: 10           # LRU cache size for recent episodes
      track_entity_mentions: true         # Track which entities appear in episodes
    
    # Rolling Summary
    # Maintains an incremental user profile summary
    summary:
      enabled: true
      refresh_interval: "5m"              # Background refresh interval
      max_facts: 50                        # Max facts to include in summary
      max_tokens: 512                      # Max tokens for generated summary
      async: true                          # Refresh in background thread
      incremental_update: true            # Delta-based updates vs full regeneration
      min_new_facts_for_update: 5         # Threshold for incremental updates
    
    # Parallel Processing
    # Efficient batch processing with goroutine pools
    parallel:
      max_workers: 0                       # Worker count (0 = auto-detect from CPU)
      batch_size: 10                       # Batch size for bulk operations
      queue_size: 100                      # Async processing queue size
      enable_async: true                   # Enable background processing
    
    # External RAG Integration
    # Use external RAG system as knowledge source (fast for large corpuses)
    external_rag:
      enabled: false                      # Enable external RAG integration
      corpus_path: ""                     # Uses RAG corpus_path if empty
      use_as_primary: false              # Use RAG instead of internal storage
      fallback_to_internal: true          # Fall back to internal if RAG returns nothing

server:
  # SECURITY: Use 127.0.0.1 (loopback) to restrict to local connections only.
  # Set to 0.0.0.0 only if you need network access AND have a firewall in place.
  host: "127.0.0.1"
  port: 8080
  enabled: true

conversation:
  system_message: ""
  template_path: ""

rag:
  enabled: false
  corpus_path: "/home/theawakener/Projects/OpenEye_00/rag_corpus"
  max_chunks: 4
  chunk_size: 512
  chunk_overlap: 64
  min_score: 0.2
  index_path: "openeye_rag.index"
  extensions: [".txt", ".md", ".markdown", ".rst", ".log", ".csv", ".tsv", ".json", ".yaml", ".yml", ".pdf"]
  
  # Hybrid Retrieval (combines semantic + keyword + diversity)
  hybrid_enabled: true                  # Enable advanced hybrid retrieval
  max_candidates: 50                    # Candidates to consider before reranking
  diversity_threshold: 0.3              # MMR diversity threshold (0-1)
  semantic_weight: 0.7                  # Weight for semantic similarity (0-1)
  keyword_weight: 0.2                   # Weight for BM25 keyword matching (0-1)
  rag_recency_weight: 0.1               # Weight for document recency (0-1)
  enable_query_expansion: true          # Expand queries with context terms
  dedupe_threshold: 0.85                # Similarity threshold for deduplication
  merge_adjacent_chunks: true           # Merge adjacent chunks from same source
  max_merged_tokens: 1000               # Max tokens when merging chunks
  
  # Early Termination (optimization for large corpora)
  early_termination: true               # Enable early termination for large corpuses
  early_term_multiplier: 3.0           # Multiplier for target results (K * multiplier)
  early_term_min_chunks: 500           # Minimum chunks before early termination kicks in

assistants:
  summarizer:
    enabled: false
    prompt: "Summarize the following conversation history in concise bullet points highlighting commitments, open questions, and facts. Respond with at most five bullets."
    max_tokens: 128
    min_turns: 3
    max_references: 8
    similarity_threshold: 0.1
    max_transcript_tokens: 0

embedding:
  enabled: true
  backend: "native"
  native:
    model_path: "models/all-MiniLM-L6-v2-Q4_K_M.gguf"
    threads: 4
  llamacpp:
    base_url: "http://127.0.0.1:42068"
    model: ""
    timeout: "0"
